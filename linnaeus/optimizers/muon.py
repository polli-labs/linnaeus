"""
Muon Optimizer Implementation

This module provides an implementation of the Muon optimizer, which is specifically designed
for 2D parameters in neural networks. It orthogonalizes the update matrix using
Newton-Schulz iterations.

This implementation follows the reference implementation by Keller Jordan:
https://kellerjordan.github.io/posts/muon/
https://github.com/KellerJordan/Muon

Muon is intended to optimize 2D parameters in neural networks by taking the updates
generated by SGD-momentum and then orthogonalizing them using Newton-Schulz iterations.
"""

import torch
import torch.distributed as dist
from torch.optim import Optimizer

from linnaeus.utils.debug_utils import check_debug_flag
from linnaeus.utils.distributed import get_rank_safely
from linnaeus.utils.logging.logger import get_main_logger

logger = get_main_logger()


def zeropower_via_newtonschulz5(G, steps=5):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G.

    This implementation follows the reference implementation exactly to ensure
    consistent behavior.

    Args:
        G: Input matrix or batched matrices to orthogonalize
        steps: Number of Newton-Schulz iterations

    Returns:
        Orthogonalized matrix
    """
    assert G.ndim >= 2
    a, b, c = (3.4445, -4.7750, 2.0315)

    # Ensure input is bfloat16
    X = G.bfloat16()

    # Handle matrices where height > width by transposing
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)

    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A
        X = a * X + B @ X

    # Transpose back if needed
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure output remains bfloat16
    return X.bfloat16()  # Explicitly return as bfloat16


class Muon(Optimizer):
    """
    Muon optimizer for 2D parameters in neural networks.

    This optimizer applies SGD with momentum and then orthogonalizes the update
    using Newton-Schulz iterations.

    IMPORTANT: This optimizer should only be used with 2D parameters or flattened 4D parameters.
    Embeddings, classifier heads, and scalar or vector parameters should be optimized using AdamW.
    """

    def __init__(
        self,
        params,
        lr=0.02,
        weight_decay=0.01,
        momentum=0.95,
        nesterov=True,
        ns_steps=5,
        strict=False,
    ):
        """
        Initialize Muon optimizer.

        Args:
            params: Iterable of parameters to optimize (must be 2D or flattened 4D)
            lr: Learning rate
            weight_decay: Weight decay (L2 penalty)
            momentum: Momentum factor
            nesterov: Whether to use Nesterov momentum
            ns_steps: Number of Newton-Schulz iterations
            strict: If True, raise error for non-2D parameters
        """
        if not 0.0 <= lr:
            raise ValueError(f"Invalid learning rate: {lr}")
        if not 0.0 <= momentum < 1.0:
            raise ValueError(f"Invalid momentum value: {momentum}")
        if not 0.0 <= weight_decay:
            raise ValueError(f"Invalid weight_decay value: {weight_decay}")

        defaults = dict(
            lr=lr,
            weight_decay=weight_decay,
            momentum=momentum,
            nesterov=nesterov,
            ns_steps=ns_steps,
        )

        # Validate parameter dimensions if strict mode is enabled
        if strict:
            for param in params:
                if param.dim() not in [2, 4]:
                    raise ValueError(
                        f"Muon optimizer requires 2D or 4D parameters, got shape {param.shape}"
                    )

        super().__init__(params, defaults)

    @torch.no_grad()
    def step(self, closure=None):
        """
        Perform a single optimization step.

        Args:
            closure: A closure that reevaluates the model and returns the loss

        Returns:
            Loss value from closure if provided
        """
        loss = None
        if closure is not None:
            with torch.enable_grad():
                loss = closure()

        for group in self.param_groups:
            weight_decay = group["weight_decay"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            lr = group["lr"]

            for p in group["params"]:
                if p.grad is None:
                    continue

                g = p.grad

                # Apply weight decay
                if weight_decay != 0:
                    p.mul_(1 - lr * weight_decay)

                # Apply momentum
                state = self.state[p]
                if "momentum_buffer" not in state:
                    state["momentum_buffer"] = torch.zeros_like(g)
                buf = state["momentum_buffer"]
                buf.lerp_(g, 1 - momentum)

                # Apply Nesterov momentum if enabled
                g = g.lerp_(buf, momentum) if nesterov else buf.clone()

                # Handle 4D parameters
                if g.ndim == 4:
                    g = g.view(g.size(0), -1)

                # Apply Newton-Schulz orthogonalization
                g = zeropower_via_newtonschulz5(g, steps=ns_steps)

                # Reshape back if needed and apply scaling factor
                if p.dim() == 4:
                    g = g.view_as(p)
                    # Apply scaling factor based on dimensions
                    scaling = (
                        max(1, p.size(0) / (p.size(1) * p.size(2) * p.size(3))) ** 0.5
                    )
                else:
                    # For 2D parameters
                    scaling = max(1, p.size(-2) / p.size(-1)) ** 0.5

                # Apply update with scaling
                p.add_(g, alpha=-lr * scaling)

        return loss


class DistributedMuon(Optimizer):
    """
    Distributed version of the Muon optimizer for multi-GPU training.

    This implementation directly follows the reference implementation approach
    to ensure stability and performance in distributed settings.
    """

    def __init__(
        self,
        params,
        lr=0.02,
        weight_decay=0.01,
        momentum=0.95,
        nesterov=True,
        ns_steps=5,
    ):
        """
        Initialize Distributed Muon optimizer.

        Args:
            params: Iterable of parameters to optimize (must be 2D or 4D)
            lr: Learning rate
            weight_decay: Weight decay (L2 penalty)
            momentum: Momentum factor
            nesterov: Whether to use Nesterov momentum
            ns_steps: Number of Newton-Schulz iterations
        """
        if not 0.0 <= lr:
            raise ValueError(f"Invalid learning rate: {lr}")
        if not 0.0 <= momentum < 1.0:
            raise ValueError(f"Invalid momentum value: {momentum}")
        if not 0.0 <= weight_decay:
            raise ValueError(f"Invalid weight_decay value: {weight_decay}")

        # Get rank and world size
        self.rank = get_rank_safely()
        self.world_size = (
            dist.get_world_size()
            if dist.is_available() and dist.is_initialized()
            else 1
        )

        defaults = dict(
            lr=lr,
            weight_decay=weight_decay,
            momentum=momentum,
            nesterov=nesterov,
            ns_steps=ns_steps,
        )

        # Convert params to list to access by index
        params_list = list(params)

        # Group parameters by size (numel) as in the reference implementation
        size_groups = {}
        for p in params_list:
            size = p.numel()
            if size not in size_groups:
                size_groups[size] = []
            size_groups[size].append(p)

        # Validate that all parameters in each size group have the same numel
        for size, group_params in size_groups.items():
            for i, p in enumerate(group_params):
                if p.numel() != size:
                    logger.error(
                        f"Parameter inconsistency detected: Expected size {size}, but got {p.numel()} for parameter {i}"
                    )

        # Create parameter groups
        param_groups = []
        for size, group_params in size_groups.items():
            # Create buffer for all_gather operations
            buffer = torch.empty(
                self.world_size, size, dtype=torch.bfloat16, device="cuda"
            )

            # Create parameter group with matching size
            group = dict(
                params=group_params,
                update_buffer=buffer,
                update_buffer_views=[buffer[i] for i in range(self.world_size)],
            )
            param_groups.append(group)

        logger.info(
            f"Created DistributedMuon with {len(param_groups)} parameter groups (rank={self.rank})"
        )

        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self, closure=None):
        """
        Perform a single optimization step.

        Args:
            closure: A closure that reevaluates the model and returns the loss

        Returns:
            Loss value from closure if provided
        """
        loss = None
        if closure is not None:
            with torch.enable_grad():
                loss = closure()

        for group in self.param_groups:
            update_buffer = group["update_buffer"]
            update_buffer_views = group["update_buffer_views"]
            params = group["params"]

            # Get optimizer settings
            weight_decay = group["weight_decay"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            lr = group["lr"]

            # Initialize state for async operations
            handle = None
            params_world = None

            # Helper function to apply updates from previous batch
            def update_prev():
                if handle is not None and params_world:
                    handle.wait()
                    for p_world, g_world in zip(params_world, update_buffer_views, strict=False):
                        # Apply weight decay directly to parameter
                        p_world.mul_(1 - lr * weight_decay)

                        try:
                            # Apply update with scaling factor
                            # Make sure g_world can be viewed as p_world
                            if g_world.numel() != p_world.numel():
                                logger.error(
                                    f"Mismatched tensor sizes: g_world.numel()={g_world.numel()}, p_world.numel()={p_world.numel()}"
                                )
                                logger.error(
                                    f"Shapes: g_world={g_world.shape}, p_world={p_world.shape}"
                                )
                                raise RuntimeError(
                                    "Mismatched tensor sizes in DistributedMuon.update_prev"
                                )

                            # Reshape g_world to match p_world's shape
                            reshaped_g = g_world.view_as(p_world)

                            # Apply the scaling factor
                            scaling = max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5

                            # Apply the update
                            p_world.add_(reshaped_g, alpha=-lr * scaling)
                        except Exception as e:
                            logger.error(f"Error in update_prev: {str(e)}")
                            logger.error(
                                f"Parameter shape: {p_world.shape}, update shape: {g_world.shape}"
                            )
                            raise

            # Process parameters in batches by rank
            for base_i in range(0, len(params), self.world_size):
                # Get this rank's parameter and compute its update
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad

                    if g is None:
                        # Create a zero tensor if grad is None
                        g = torch.zeros_like(
                            p.data.flatten(), dtype=torch.bfloat16, device="cuda"
                        )
                        if hasattr(self, "config") and check_debug_flag(
                            self.config, "DEBUG.OPTIMIZER"
                        ):
                            logger.debug(
                                f"[DistributedMuon.step] Created zero tensor with shape {g.shape} and dtype {g.dtype}"
                            )
                    else:
                        # Apply momentum
                        state = self.state[p]
                        if "momentum_buffer" not in state:
                            state["momentum_buffer"] = torch.zeros_like(g)
                        buf = state["momentum_buffer"]
                        buf.lerp_(g, 1 - momentum)

                        # Apply Nesterov momentum if enabled
                        g = g.lerp_(buf, momentum) if nesterov else buf

                        # Handle 4D parameters by flattening
                        if g.ndim == 4:
                            g = g.view(g.size(0), -1)

                        # Apply Newton-Schulz orthogonalization
                        g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    # Use buffer data for ranks that don't have a parameter
                    g = update_buffer_views[self.rank]

                # Apply updates from previous iteration
                if base_i > 0:
                    update_prev()

                # Ensure g has same dtype as update_buffer
                if g.dtype != update_buffer.dtype:
                    logger.debug(
                        f"Converting gradient from {g.dtype} to {update_buffer.dtype}"
                    )
                    g = g.to(dtype=update_buffer.dtype)

                # Gather updates from all processes
                try:
                    handle = dist.all_gather_into_tensor(
                        update_buffer, g, async_op=True
                    )
                except TypeError:
                    logger.error(
                        f"Type mismatch in all_gather_into_tensor: update_buffer.dtype={update_buffer.dtype}, g.dtype={g.dtype}"
                    )
                    logger.error(
                        f"Tensor shapes: update_buffer={update_buffer.shape}, g={g.shape}"
                    )
                    logger.error(
                        f"Parameter info: p.shape={p.shape if base_i + self.rank < len(params) else 'N/A'}"
                    )
                    raise

                # Set parameters for this batch
                end_idx = min(base_i + self.world_size, len(params))
                params_world = params[base_i:end_idx]

            # Apply final updates
            update_prev()

        return loss


def flatten_conv_parameters(params):
    """
    Create views of 4D convolutional parameters as flattened 2D parameters.

    This is a simpler version that just returns views, not new parameters.

    Args:
        params: List of parameters to potentially flatten

    Returns:
        List of 2D views of the parameters
    """
    flattened = []
    for p in params:
        if p.dim() == 4:
            # Create a view of the parameter with flattened shape
            flat_p = p.view(p.size(0), -1)
            flattened.append(flat_p)
        else:
            flattened.append(p)
    return flattened
