# configs/model/archs/mFormerV1/mFormerV1_lg.yaml
MODEL:
  TYPE: "mFormerV1"
  NAME: "mFormerV1_lg"
  # PRETRAINED_SOURCE: 'stitched_convnext_ropevit'
  # PRETRAINED_CONVNEXT: "/path/to/checkpoints/mFormerV1/convnext_large_22k_1k_384.pth"
  # PRETRAINED_ROPEVIT: "/path/to/checkpoints/mFormerV1/rope_mixed_deit_base_patch16_LS.pth"

  DROP_RATE: 0.0
  DROP_PATH_RATE: 0.4 # Match ConvNeXt-L
  ATTN_DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  ONLY_LAST_CLS: false
  EXTRA_TOKEN_NUM: 4 # 1 CLS + 3 meta components assumed
  IN_CHANS: 3
  USE_FLASH_ATTN: true # Enable Flash Attention by default

  CONVNEXT_STAGES:
    DEPTHS: [3, 3, 27, 3]   # Depths from ConvNeXt-L
    DIMS:   [192, 384, 768, 1536] # Dims from ConvNeXt-L
    LAYER_SCALE_INIT_VALUE: 1.0e-6

  ROPE_STAGES:
    DEPTHS: [10, 2]          # Use all 12 layers from DeiT-B, split like _md
    DIMS:   [768, 1536]      # Matches ConvNeXt-L DIMS[2] and DIMS[3]
    NUM_HEADS: [12, 24]      # From DeiT-B (768) and extrapolated (1536)
    MLP_RATIO: [4.0, 4.0]
    ROPE_THETA: 10000.0
    ROPE_MIXED: True

  AGGREGATION: # Adjust in_channels
    TYPE: "Conv1d"
    PARAMETERS:
      in_channels: 1536 # Matches final RoPE dim
      out_channels: 1536
      kernel_size: 2
      bias: true

  CLASSIFICATION:
    HEADS: {}

  NORMALIZATION:
    CONV_NORM_LAYER: "LayerNormChannelsFirst"
    ATTENTION_NORM_LAYER: "LayerNorm"
    ACTIVATION_LAYER: "GELU"