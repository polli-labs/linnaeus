# mFormerV0_lg.yaml - Large variant matching MetaFG_2
MODEL:
  TYPE: "mFormerV0"
  NAME: "mFormerV0_lg"

  DROP_RATE: 0.0
  DROP_PATH_RATE: 0.3  # Highest drop path for largest model
  LABEL_SMOOTHING: 0.1

  ONLY_LAST_CLS: false
  EXTRA_TOKEN_NUM: 1

  IN_CHANS: 3

  # -----------------------------------------------------------
  # Convolutional Stages:
  #   S0 => 3-layer stem => out=128 (2x channels vs sm/md)
  #   S1 => 2 MBConv blocks => out=128 (wider channels)
  #   S2 => 6 MBConv blocks => out=256 (deeper + wider vs sm)
  # -----------------------------------------------------------
  CONV_STAGES:
    STEM_OUT: 128         # 2x wider stem
    EMBED_DIMS:   [128, 128]   # stage1 in/out: 128→128 
    OUT_CHANNELS: [128, 256]   # stage2 in/out: 128→256
    DEPTHS:       [2,   6]     # stage1 => 2 blocks, stage2 => 6 blocks
    #
    # stage1 => 2 blocks => [1,1] (no further downsample)
    # stage2 => 6 blocks => [1,1,1,1,1,1] (all maintain resolution)
    # 
    STRIDE_SEQS:
      - [2, 1]
      - [1, 1, 1, 1, 1, 1]

  # -----------------------------------------------------------
  # Attention Stages: 
  #   S3 => 14 Transformer blocks => embed=512 (wider vs sm/md)
  #   S4 => 2 Transformer blocks => embed=1024 (wider channels)
  # -----------------------------------------------------------
  ATTENTION_STAGES:
    EMBED_DIMS: [512, 1024]  # 1.33x/1.33x wider vs sm/md
    DEPTHS: [14, 2]          # Same depths as md variant
    NUM_HEADS: [8, 8]
    MLP_RATIO: [4.0, 4.0]
    ATTENTION_TYPE: ["RelativeAttention", "RelativeAttention"]
    #
    # stage3 => 14 blocks => first block has stride=2 for downsampling
    # stage4 => 2 blocks => first block has stride=2 for downsampling
    #
    STRIDE_SEQS:
      - [2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  # stage3: downsample once
      - [2, 1]  # stage4: downsample once

  AGGREGATION:
    TYPE: "Conv1d"
    PARAMETERS:
      in_channels: 1024    # Matches final embed dim
      out_channels: 1024
      kernel_size: 2 
      bias: true

  CLASSIFICATION:
    HEADS: {}

  NORMALIZATION:
    CONV_NORM_LAYER: "BatchNorm2d"
    ATTENTION_NORM_LAYER: "LayerNorm"
    ACTIVATION_LAYER: "ReLU"